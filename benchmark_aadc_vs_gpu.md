# AADC vs GPU Benchmark Runs

Auto-generated by `benchmark_aadc_vs_gpu.py` and updated manually with findings from batched line search benchmarks and cross-backend consistency verification.

---

## Issues Found and Fixed (2026-02-02)

### Problem: GPU IM Mismatch on Multi-Asset Portfolios

When running mixed trade types (ir_swap + equity_option + fx_option), the GPU and AADC backends produced different IM values (~5% discrepancy). The GPU gradient could not find IM improvements while AADC could.

**Root causes and fixes:**

| # | Issue | Symptom | Fix |
|---|-------|---------|-----|
| 1 | **AADC missing inter-bucket gamma for non-Rates risk classes** | AADC kernel only implemented `ir_gamma_diff_ccy` for Rates. CreditQ, CreditNonQ, Equity, and Commodity used gamma=0 (no inter-bucket aggregation). GPU kernel handled all risk classes correctly. | Added gamma lookup tables for all 5 risk classes in `simm_portfolio_aadc_v2.py`: `ir_gamma_diff_ccy`, `_CQ_INTER` (creditQ_corr_non_res 12x12), `cr_gamma_diff_ccy`, `_EQ_INTER` (equity_corr_non_res 12x12), `_CM_INTER` (commodity_corr_non_res 17x17). |
| 2 | **AADC NaN gradients from sqrt of negative k_rc_sq** | Inter-bucket cross-terms `S_b * gamma * S_c` can be negative, making `k_rc_sq < 0`. `sqrt(negative)` produces NaN that propagates through AADC adjoint. GPU kernel guarded with `if margin > 0 else 0`. | Added smooth `max(0, x)` approximation at all three sqrt sites: `k_sq_safe = (k_sq + sqrt(k_sq * k_sq + eps)) / 2 + eps` with `eps = 1.0` ($1 floor, negligible vs $1e15 IM). |
| 3 | **GPU MAX_B overflow** | Mixed trade types produced 69 buckets across all risk classes, exceeding `MAX_B = 64` in GPU kernel. GPU wrote out of bounds, producing garbage IM ($2.8e27). | Increased `MAX_B` from 64 to 128 in `simm_portfolio_cuda.py`. |

### Files Modified

| File | Change |
|------|--------|
| `model/simm_portfolio_aadc_v2.py` | Added inter-bucket gamma for all 5 risk classes; added smooth max at 3 sqrt sites; imported `creditQ_corr_non_res`, `equity_corr_non_res`, `commodity_corr_non_res`, `cr_gamma_diff_ccy` |
| `model/simm_portfolio_cuda.py` | `MAX_B = 64` → `MAX_B = 128` |

### After Fixes: Verification Results

| Metric | Value |
|--------|-------|
| IM relative difference (AADC vs GPU) | 3.12e-15 (machine precision) |
| NaN gradients | 0 |
| Max gradient relative error (valid entries) | 9.2e-15 |
| Optimization result (both backends) | 4.2% IM reduction (P=10, mixed types) |

Both backends now produce identical IMs and gradients to machine precision, and both find the same optimization improvements.

---

## Run: 2026-02-02 12:56:33 — Simplified SIMM (Pre-Fix, Historical)

> **Note**: This run used the old kernel before inter-bucket gamma fixes. GPU IM was $0 due to missing risk-class support. Results are preserved for reference only.

### Configuration

| Parameter | Value |
|-----------|-------|
| Command | `python benchmark_aadc_vs_gpu.py --trades 100 --portfolios 3 --threads 4` |
| Trades | 100 |
| Portfolios | 3 |
| Trade types | ir_swap |
| Threads (AADC) | 4 |
| Risk factors (K) | 30 |
| Sensitivity matrix | 100 x 30 |

### IM Values

| Backend | Total IM | Match |
|---------|----------|-------|
| AADC (CPU AAD) | $203,084,979,729.35 | Baseline |
| GPU (CUDA) | $0.00 | NO (diff: $96,563,757,293) |

### Gradient Accuracy

| Metric | Value |
|--------|-------|
| Max abs diff | 4.27e+01 |
| Max rel diff | 1.00e+00 |
| Match | NO |

---

## GPU Architecture Assessment

### Current Problem: Parallelizing Over the Wrong Dimension

The current GPU kernel (`simm_portfolio_cuda.py`) launches **one thread per portfolio**:

```
threads_per_block = 256
blocks = (P + 255) // 256
```

With a realistic P=5-20 portfolios, this gives grid size 1 (one block of 256 threads, only 5-20 active). Numba warns: `Grid size 1 will likely result in GPU under-utilization due to low occupancy`. An H100 has 132 SMs and supports >100K concurrent threads — using 5 of them wastes >99.99% of the chip.

This is not fixable with parameters. **In production, you don't increase the number of portfolios to fill GPU cores.** The parallelization strategy itself is wrong.

### What Each Thread Does (the real work)

Per portfolio, the kernel runs these sequential loops:

| Step | Operation | Complexity | Description |
|------|-----------|------------|-------------|
| 1 | Weighted sensitivities | O(K) | `WS[k] = S[k] * RW[k] * CR[k]` |
| 2 | Bucket sums | O(K) | Accumulate WS into B buckets |
| 3 | **Intra-bucket correlations** | **O(K²)** | `K_b² = Σ_kl ρ_kl × WS_k × WS_l` |
| 4 | Inter-bucket gamma | O(B²) | `Σ γ_bc × S_b × S_c` per risk class |
| 5 | Cross-RC aggregation | O(36) | 6×6 PSI matrix multiply |
| 6 | **Gradient per factor** | **O(K² + KB)** | Chain rule back through steps 3-5 |

With K=108 factors, steps 3 and 6 dominate: ~12K multiply-adds per portfolio for the forward pass, ~12K per factor for the gradient (~1.3M ops total). This is substantial work — but it's all done in a single thread.

### How to Actually Benefit from GPU

The SIMM formula is fundamentally linear algebra. A Python developer with a NumPy implementation has three options, from easiest to hardest:

#### Option A: Drop-in CuPy replacement (minimal changes)

Replace `import numpy as np` with `import cupy as cp`. The SIMM formula maps directly to matrix operations:

```python
# CPU (current)
WS = sensitivities * weights * concentration            # (P, K) element-wise
K_b_sq = WS @ intra_corr @ WS.T                        # (P, P) quadratic form
# ... extract diagonal per bucket ...

# GPU (CuPy — same API, runs on GPU)
WS = cp.asarray(sensitivities) * cp.asarray(weights) * cp.asarray(concentration)
K_b_sq = WS @ cp.asarray(intra_corr) @ WS.T
```

**What changes**: ~5 lines (array transfer to/from GPU). No custom kernels.
**What doesn't change**: Algorithm, formula, code structure.
**Limitation**: Python loop over buckets/risk classes remains serial on CPU, only the matrix ops inside each iteration run on GPU. Fine for K>100 where matmul dominates.

#### Option B: Restructure as batched matrix operations (moderate changes)

Reorganize the per-bucket loops into batched operations across all buckets simultaneously:

```python
# Instead of looping over B buckets:
for b in buckets:
    K_b_sq[b] = WS_b.T @ rho_b @ WS_b        # one matmul per bucket

# Batch all buckets into one operation:
# WS_blocked[b] is (P, K_b) for bucket b — pad to max bucket size
# rho_blocked[b] is (K_b, K_b)
# Use np.einsum or block-diagonal matmul:
K_all_sq = np.einsum('bpi,bij,bpj->bp', WS_blocked, rho_blocked, WS_blocked)
```

**What changes**: Loop structure. Buckets computed in parallel instead of sequentially.
**Benefit**: Eliminates the Python loop over B=40-70 buckets. All correlation work in one GPU call.
**Effort**: Requires padding/masking for variable-size buckets, restructuring the bucket grouping.

#### Option C: Custom CUDA kernel with proper parallelism (significant rewrite)

Parallelize over `(portfolio, factor)` pairs or `(portfolio, bucket)` pairs:

```
# Current: P threads (1 per portfolio, all K² work sequential)
grid = (P,)

# Better: P × B threads (1 per portfolio × bucket)
grid = (P, B)
# Each thread does O(K_b²) for its assigned bucket
# Reduction across buckets for inter-bucket gamma

# Best: P × K threads with shared memory
grid = (P, K)
# Each thread handles one factor's contribution
# Shared memory for bucket accumulation and reduction
```

**What changes**: Everything — new kernel, new launch config, shared memory management.
**Benefit**: Full GPU utilization regardless of P. K=108 factors × P=5 portfolios = 540 threads (still small, but workable with enough work per thread).
**Effort**: Substantial. Must manually handle reductions, bank conflicts, warp divergence.

### Recommendation: What a Python Developer Should Do

| Scenario | Approach | Effort | Speedup Expectation |
|----------|----------|--------|---------------------|
| K < 50, P < 20 | **Stay on CPU** | None | GPU overhead > benefit |
| K > 100, any P | **Option A (CuPy)** | Low | 2-10x for matmul-heavy parts |
| K > 100, optimization loop (many evals) | **Option B (batched)** | Medium | 5-20x (eliminates Python loops) |
| Full production system | **Option C (custom kernel)** | High | 10-50x (but diminishing returns vs AADC) |

**Key insight**: For SIMM specifically, AADC on CPU is likely the better investment than GPU. AADC gives you exact gradients via adjoint mode in a single forward+backward pass, with O(K) cost for ALL K gradients. The GPU kernel computes gradients via manual chain rule at O(K²) cost. AADC's algorithmic advantage (O(K) vs O(K²) for gradients) often outweighs GPU's hardware advantage, especially at moderate K.

**Where GPU genuinely wins**: The optimization loop. If you're running 1000+ SIMM evaluations (greedy search, stress testing, what-if analysis), parallelizing the candidates across GPU cores — evaluating 100 candidate trade moves simultaneously — is where GPU adds real value. This is a different kernel than the SIMM computation itself.

### Brute-Force GPU Without Gradients

Yes — this is arguably the strongest case for GPU in SIMM optimization.

#### Why it works

The current greedy optimizer (`greedy_local_search`) does this per round:
1. Compute gradient (1 eval with gradient) → rank candidate moves
2. Try top-k moves **one at a time**, each requiring a full P-portfolio SIMM eval
3. Accept or revert each move sequentially

With T=10K trades and P=20 portfolios, there are **T × (P-1) = 190,000** possible single-trade moves. The current approach uses gradient to filter down to ~1000 candidates, then evaluates them serially. A brute-force GPU approach evaluates **all 190K candidates in one kernel launch** — no gradient needed.

#### The incremental trick

Moving trade `t` from portfolio A to portfolio B only changes 2 of P portfolios:
```
new_agg_S[A, :] = old_agg_S[A, :] - S[t, :]    # remove trade from source
new_agg_S[B, :] = old_agg_S[B, :] + S[t, :]    # add trade to dest
delta_IM = (new_IM_A + new_IM_B) - (old_IM_A + old_IM_B)
```

Each GPU thread evaluates ONE candidate move:
- Thread ID encodes `(trade_t, dest_portfolio)` — 190K threads total
- Compute 2 incremental aggregations: O(K) each
- Run **forward-only** SIMM for 2 portfolios: O(K²) each
- Output: `delta_IM` for this move
- No gradient computation → kernel is ~half the size and cost

#### Performance estimate (K=108, T=10K, P=20)

| | Current serial greedy | Brute-force GPU |
|---|---|---|
| Candidates evaluated per round | ~1000 (gradient-filtered) | 190,000 (all possible) |
| SIMM evals per candidate | P=20 portfolios (full) | 2 portfolios (incremental) |
| Work per candidate | ~12K ops × 20 = 240K ops | ~12K ops × 2 = 24K ops |
| Parallelism | 1 (sequential) | 190,000 threads |
| Needs gradient? | Yes (to rank candidates) | No |
| Grid size | 1 | ~750 blocks × 256 threads |
| Rounds to converge | ~10-50 | ~10-50 (same algorithm) |
| Estimated time per round | ~seconds (serial evals) | ~1ms (one kernel launch) |

#### Why this is the right GPU architecture for SIMM

1. **Grid size 190K** — full GPU utilization (vs grid size 1 with current approach)
2. **Forward-only kernel** — half the code, half the registers, half the memory
3. **No gradient pitfalls** — no NaN from sqrt, no smooth-max hacks, no AADC dependency
4. **Globally optimal per round** — evaluates ALL moves, not a gradient-filtered subset (gradient can miss beneficial moves due to non-convexity)
5. **Simpler code** — just the forward SIMM formula in a kernel, no chain rule
6. **Incremental eval** — only recompute 2 affected portfolios, not all P

#### What the kernel looks like

```python
@cuda.jit
def eval_all_moves_kernel(
    agg_S,           # (P, K) current aggregated sensitivities
    S,               # (T, K) per-trade sensitivities
    curr_assign,     # (T,) current portfolio assignment per trade
    base_im,         # (P,) current IM per portfolio
    risk_weights,    # (K,) ...SIMM constants...
    delta_im_out,    # (T, P) output: IM change for each possible move
):
    idx = cuda.grid(1)
    t = idx // (P - 1)       # which trade
    move_idx = idx % (P - 1) # which destination (skip current)
    src_p = curr_assign[t]
    dst_p = move_idx if move_idx < src_p else move_idx + 1

    # Incremental aggregation
    new_src_agg = agg_S[src_p, :] - S[t, :]
    new_dst_agg = agg_S[dst_p, :] + S[t, :]

    # Forward-only SIMM (no gradient)
    new_src_im = simm_forward(new_src_agg, ...)
    new_dst_im = simm_forward(new_dst_agg, ...)

    delta_im_out[t, dst_p] = (new_src_im + new_dst_im) - (base_im[src_p] + base_im[dst_p])
```

Then on CPU: `best_move = np.argmin(delta_im_out)` → apply move → repeat.

#### When to use gradient vs brute-force

| Approach | Best when | Weakness |
|----------|-----------|----------|
| Gradient (AADC) | K is large, continuous relaxation works | Requires rounding; misses discrete structure |
| Gradient (GPU manual) | Never — O(K²) gradient on GPU < O(K) gradient via AADC | Worst of both worlds |
| **Brute-force GPU** | **T×P fits in GPU grid (>1000 threads), forward pass is cheap** | **Memory for delta_im_out (T×P floats)** |
| Hybrid | Large T: use gradient to filter to top-N, then GPU-eval top-N | More complex pipeline |

**Bottom line**: Brute-force GPU is the natural fit for SIMM allocation optimization. It plays to GPU strengths (massive parallelism, simple uniform computation) and avoids GPU weaknesses (complex gradient code, small grid sizes). A Python developer should write a forward-only SIMM kernel and parallelize over candidate moves, not over portfolios.

### Summary of Required Logic Changes

| Component | Current (per-portfolio thread) | GPU-native approach | Rethink needed? |
|-----------|-------------------------------|--------------------|----|
| `WS = S * RW * CR` | Sequential loop O(K) | Element-wise GPU op | No — trivial |
| Bucket sum `S_b` | Sequential loop O(K) | Scatter-add or segment-reduce | Minor |
| **Intra-bucket `K_b²`** | **Nested loop O(K²)** | **Batched matmul across buckets** | **Yes — restructure** |
| Inter-bucket gamma | Loop O(B²) | Small matmul (B×B) | No — too small to matter |
| Cross-RC PSI | Loop O(36) | 6×6 matmul | No — trivial |
| **Gradient** | **Manual chain rule O(K²)** | **Use AADC instead, or batched Jacobian** | **Yes — fundamental** |

The intra-bucket correlation (O(K²)) and gradient computation need restructuring. Everything else maps directly. A Python developer would NOT need to rethink the SIMM formula itself — only the loop structure around it.

---

## Batched Line Search Benchmark Results (2026-02-02)

### Concept

The current serial optimizer evaluates ONE step size per line search trial (up to 10 sequential kernel calls per iteration). The batched version pre-computes ALL candidate step sizes and evaluates them in a SINGLE kernel call by treating each `(step_size, portfolio)` pair as an independent portfolio in the batch.

- **Serial**: 1 gradient eval + up to 10 serial LS evals = ~11 kernel calls/iteration
- **Batched**: 1 gradient eval + 1 batched LS eval (candidates x P portfolios) = 2 kernel calls/iteration

### Implementation

| File | Purpose |
|------|---------|
| `model/batched_optimizer.py` | Independent batched line search optimizer (v1.0.0) |
| `benchmark_batched_linesearch.py` | A/B benchmark: serial vs batched, AADC Py + GPU (v1.0.0) |

### Results: T=10,000 trades, P=30 portfolios, Adam optimizer, 16 LS candidates

| Backend | Serial | Batched | Speedup |
|---------|--------|---------|---------|
| AADC Py | 222.29 ms | 2.696 s | 0.08x |
| GPU | 920.64 ms | 2.835 s | 0.32x |

### Results: T=100,000 trades, P=300 portfolios, Adam optimizer, 16 LS candidates

| Backend | Serial | Batched | Speedup |
|---------|--------|---------|---------|
| AADC Py | 4.332 s | 44.854 s | 0.10x |
| GPU | 4.748 s | 44.984 s | 0.11x |

### Analysis

In these runs, the optimizer converged in only 2 iterations with 0 trades moved (ir_swap-only portfolios have identical risk profiles, preventing netting). With so few iterations and early convergence, the serial line search's early-exit behavior is more efficient than evaluating all candidates.

**Key finding**: Batching helps most when:
1. The optimizer runs many iterations with frequent line search retries
2. GPU kernel launch latency dominates (small P) — observed 24x speedup at P=3 on GPU
3. Mixed trade types create real netting opportunities, forcing more optimizer iterations

**Where batching does NOT help**:
- AADC Python: compute scales linearly with batch size, dispatch overhead is negligible
- Large P with early convergence: serial early-exit saves most of the work

### Recommendation

For production use, the serial line search with Armijo early exit remains the better default. Batched line search is beneficial specifically for GPU workloads with small portfolio counts (P < 10) where kernel launch latency is the bottleneck.

---

## Trading Day Workflow: 4-Backend Benchmark (2026-02-02)

### Overview

The trading workflow (`benchmark_trading_workflow.py` v2.8.0) simulates a full trading day in 5 steps, with up to 4 compute backends measured per step:

| Backend | Code | Description |
|---------|------|-------------|
| **AADC Python** | `aadc_full` | Python AADC kernel: single `aadc.evaluate()` for all P portfolios, gradient via adjoint |
| **GPU CUDA** | `gpu_full` | Numba CUDA kernel: handwritten SIMM + analytical gradient |
| **Brute-Force GPU** | `bf_gpu` | Forward-only CUDA kernel: no gradients, enumerates candidates |
| **C++ AADC** | `cpp_aadc` | C++ binary (`build/simm_optimizer`): AADC SDK with OpenMP threading |

### Workflow Steps

| Step | Name | Time | What it does | Backends |
|------|------|------|-------------|----------|
| 1 | Portfolio Setup | 9:00 AM | Compute IM for initial allocation | AADC, GPU, BF, C++ |
| 2 | Margin Attribution | 10:00 AM | Euler-allocated IM contribution per trade | AADC, GPU, BF, C++ |
| 3 | Pre-Trade Routing | 1:00 PM | Route new trades to best counterparty | AADC, GPU, BF, C++ |
| 4 | What-If Scenarios | 3:00 PM | Unwind, stress IR/EQ, marginal IM | AADC, GPU, BF, C++ |
| 5 | EOD Optimization | 5:00 PM | Reallocate trades to minimize total IM | Per method (see below) |

### EOD Optimization Methods

Step 5 runs up to 3 optimization methods (controlled by `--exclude`):

| Method | Step Name | Phase 1 | Phase 2 | Backends |
|--------|-----------|---------|---------|----------|
| `gradient_descent` | EOD: GD | Continuous GD with Armijo line search | Greedy local search | AADC, GPU, C++ |
| `adam` | EOD: Adam | Adam optimizer (β1=0.9, β2=0.999) | Greedy local search | AADC, GPU, C++ |
| `gpu_brute_force` | EOD: Brute-Force | Enumerate all T×(P-1) single-trade moves | Accept best per round | BF |

### Brute-Force Implementation Per Step

| Step | BF Approach | GPU Launches | Complexity |
|------|-------------|-------------|------------|
| 1 (Setup) | Forward-only IM eval | 1 | O(P×K²) |
| 2 (Attribution) | Bump-and-revalue: remove each trade, recompute IM | 1 (batched) | O(T×K²) |
| 3 (Pre-Trade) | Try all P portfolios for each new trade | 2 per trade | O(N×P×K²) |
| 4 (What-If) | Forward-only scenario evaluation | 1 per scenario | O(S×K²) |
| 5 (Brute-Force) | Evaluate all T×(P-1) single-trade moves per round | 1 per round | O(R×T×P×K²) |

### CSV Logging

Each run produces rows in `data/execution_log_portfolio.csv`. Row count per run:
- Steps 1-4: up to 4 rows each (aadc, gpu, bf, cpp — only if backend ran)
- EOD GD: 3 rows (aadc, gpu, cpp)
- EOD Adam: 3 rows (aadc, gpu, cpp)
- EOD Brute-Force: 1 row (bf)
- **Total: up to 23 rows per run** (22 without exclusions on a machine with all backends)

### C++ AADC Backend

The C++ binary (`build/simm_optimizer`) supports both GD and Adam optimization via `--method`. The Python workflow invokes it with `--mode all` for each method:

1. **GD run**: `simm_optimizer --mode all --method gradient_descent --input-dir <shared_data>` — parses attribution, whatif, pretrade, and GD optimize
2. **Adam run**: `simm_optimizer --mode all --method adam --input-dir <shared_data>` — only optimize section used (attribution/whatif/pretrade identical)

Both runs share the same exported sensitivity matrix and allocation from the Python workflow (`data/shared_benchmark_data/`), ensuring apples-to-apples comparison.

### C++ Adam Optimizer (`allocation_optimizer.h`)

The C++ Adam implementation (lines 604-733) uses:
- Hyperparameters: β1=0.9, β2=0.999, ε=1e-8
- Bias-corrected moments: m̂ = m/(1-β1^t), v̂ = v/(1-β2^t)
- Adaptive step: lr × m̂ / (√v̂ + ε)
- Backtracking line search on final step (β=0.5, up to 10 retries)
- Early stopping after 20 consecutive stalling iterations
- Simplex projection per row (sum=1, all≥0)
- Greedy refinement on integer allocation (top-k gradient-guided moves)

### Usage

```bash
source venv/bin/activate

# Default: all 3 methods, all backends
python benchmark_trading_workflow.py --trades 1000 --portfolios 5

# Exclude brute-force
python benchmark_trading_workflow.py --trades 1000 --portfolios 5 --exclude gpu_brute_force

# Exclude Adam and brute-force (GD only)
python benchmark_trading_workflow.py --trades 1000 --portfolios 5 --exclude adam gpu_brute_force

# Quick test with minimal output
python benchmark_trading_workflow.py --trades 100 --portfolios 3 --output none
```
